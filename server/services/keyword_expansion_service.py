from __future__ import annotations

import json
import logging
import re
from typing import Dict, List, Optional, Sequence

from ai_agents.llm.gemini_client import GeminiClient, GeminiError

logger = logging.getLogger(__name__)


class KeywordExpansionService:
    """
    Lightweight helper that asks the LLM for related keywords and phrases
    so the frontend can suggest richer search terms without uploading PDFs.
    """

    def __init__(self, client: Optional[GeminiClient] = None) -> None:
        self._client = client or GeminiClient()
        self._text = self._client.text

    def expand_keywords(
        self,
        keywords: Sequence[str],
        *,
        language: str = "en",
        hints: Optional[str] = None,
        max_terms: int = 10,
    ) -> Dict[str, object]:
        unique = _unique_terms(keywords)
        if not unique:
            raise ValueError("At least one keyword is required.")

        capped_terms = max(3, min(max_terms, 20))
        prompt = self._build_prompt(unique, language=language, hints=hints, limit=capped_terms)

        try:
            raw = self._text.chat(prompt, temperature=0.35, max_output_tokens=2048)
        except (GeminiError, ValueError) as exc:
            logger.warning("Keyword expansion failed via Gemini: %s", exc)
            return self._fallback(unique, hints=hints, limit=capped_terms, reason=str(exc))

        parsed = self._parse_response(raw, language=language)
        if parsed["keywords"]:
            return parsed

        logger.info("Keyword expansion returned empty payload; using fallback.")
        return self._fallback(unique, hints=hints, limit=capped_terms, reason="Empty LLM response")

    def _build_prompt(self, keywords: Sequence[str], *, language: str, hints: Optional[str], limit: int) -> str:
        keywords_line = ", ".join(keywords)
        hints_block = hints.strip() if hints else ""
        return (
            "You are assisting with academic search planning.\n"
            "Given the seed keywords, suggest related, more specific, and broader terms "
            "that would help locate relevant papers.\n"
            f"Seed keywords: {keywords_line}\n"
            f"Additional context: {hints_block or 'N/A'}\n"
            f"Return JSON with fields 'keywords' (array of up to {limit} keyword/phrase suggestions)"
            " and 'reasoning' (one short paragraph in the requested language explaining the grouping).\n"
            f"Language: {language}.\n"
            "Keep keywords concise (1-4 words) and omit duplicates or overly generic phrases."
        )

    def _parse_response(self, text: str, *, language: str) -> Dict[str, object]:
        text = text.strip()
        keywords: List[str] = []
        reasoning = ""

        if not text:
            return {"keywords": keywords, "reasoning": reasoning, "language": language}

        try:
            payload = json.loads(text)
            extracted = payload.get("keywords")  # type: ignore[attr-defined]
            if isinstance(extracted, list):
                keywords = _unique_terms(extracted)
            elif isinstance(extracted, str):
                keywords = _unique_terms(extracted.split(","))
            reasoning_raw = payload.get("reasoning")
            if isinstance(reasoning_raw, str):
                reasoning = reasoning_raw.strip()
        except (json.JSONDecodeError, AttributeError):
            pass

        if not keywords:
            # Attempt to split lines like "1. keyword"
            for line in text.splitlines():
                cleaned = re.sub(r"^\s*[\-\*\d\.\)]\s*", "", line).strip()
                if not cleaned:
                    continue
                if ":" in cleaned and len(cleaned.split()) <= 2:
                    # Likely reasoning fragments (e.g., "Background: ...")
                    continue
                if len(cleaned) <= 60:
                    keywords.append(cleaned)
            keywords = _unique_terms(keywords)

        if not reasoning and len(text) < 400:
            reasoning = text.strip()
        elif not reasoning:
            reasoning = "Related keywords were generated by the AI model."

        return {"keywords": keywords, "reasoning": reasoning, "language": language}

    def _fallback(
        self,
        keywords: Sequence[str],
        *,
        hints: Optional[str],
        limit: int,
        reason: str,
    ) -> Dict[str, object]:
        suffixes = ["analysis", "applications", "limitations", "datasets", "benchmarks", "tooling", "evaluation"]
        generated: List[str] = []
        for idx, keyword in enumerate(keywords):
            base = keyword.strip()
            if base:
                generated.append(base)
                if idx < len(suffixes):
                    generated.append(f"{base} {suffixes[idx]}")
        generated.extend(suffixes)
        trimmed = _unique_terms(generated)[:limit]
        reasoning = "Fallback keyword suggestions generated locally due to: " + reason
        if hints:
            reasoning += f" Context: {hints.strip()}"
        return {"keywords": trimmed, "reasoning": reasoning, "language": "en"}


def _unique_terms(values: Sequence[str]) -> List[str]:
    seen: set[str] = set()
    ordered: List[str] = []
    for value in values:
        text = str(value or "").strip()
        if not text:
            continue
        lower = text.lower()
        if lower in seen:
            continue
        seen.add(lower)
        ordered.append(text)
    return ordered
